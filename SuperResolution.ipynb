{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5195bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as opt\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "from torch.cuda import amp\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9fe148",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "dataset_filepath = \"/Volumes/Danjie HDD/Imagenet/ILSVRC/Data/CLS-LOC/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e22bdb",
   "metadata": {},
   "source": [
    "# Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be84372",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel: int, out_channel: int, up: bool = False):\n",
    "        super().__init__()\n",
    "\n",
    "        # Upsampling or downsampling only for skip connection\n",
    "        self.up = up\n",
    "\n",
    "        # Normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(in_channel)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channel)\n",
    "\n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv2d(in_channel, out_channel, kernel_size=3, padding=1)\n",
    "        self.conv2 = self.zero_out(nn.Conv2d(out_channel, out_channel, kernel_size=3, padding=1))\n",
    "\n",
    "        # Skip connection\n",
    "        if in_channel != out_channel or up:\n",
    "            self.skip_connection = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size=1),\n",
    "                nn.Upsample(scale_factor=2) if up else nn.Identity(),\n",
    "            )\n",
    "        else:\n",
    "            self.skip_connection = nn.Identity()\n",
    "\n",
    "    def zero_out(self, layer: nn.Module) -> nn.Module:\n",
    "        \"\"\"\n",
    "        Zero out the parameters of a layer.\n",
    "\n",
    "        :param layer: The layer to zero out.\n",
    "        :return: The zeroed layer.\n",
    "        \"\"\"\n",
    "        for p in layer.parameters():\n",
    "            p.detach().zero_()\n",
    "        return layer\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        skip_tensor = self.skip_connection(tensor)\n",
    "\n",
    "        # Main path\n",
    "        tensor = self.norm1(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        if self.up:\n",
    "            tensor = F.interpolate(tensor, scale_factor=2)\n",
    "        tensor = self.conv1(tensor)\n",
    "        tensor = self.norm2(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = self.conv2(tensor)\n",
    "\n",
    "        tensor += skip_tensor\n",
    "        return tensor\n",
    "\n",
    "\n",
    "class SelfAttentionBlock(nn.Module):\n",
    "    def __init__(self, embedding_dim: int, image_size: int, head_dim: int = 64, channel_per_group: int = 16):\n",
    "        super().__init__()\n",
    "        self.head_dim: int = head_dim\n",
    "        self.num_head: int = embedding_dim // head_dim\n",
    "        self.scale: float = head_dim ** -0.5\n",
    "        self.num_pixel = image_size ** 2\n",
    "        self.gnorm1 = nn.GroupNorm(embedding_dim // channel_per_group, embedding_dim)\n",
    "        self.gnorm2 = nn.GroupNorm(embedding_dim // channel_per_group, embedding_dim)\n",
    "\n",
    "        # QKV projection\n",
    "        self.qkv_proj = nn.Linear(embedding_dim, embedding_dim * 3)\n",
    "\n",
    "        # Output layer\n",
    "        self.output = nn.Conv2d(embedding_dim, embedding_dim, kernel_size=1)\n",
    "\n",
    "        # Positional embedding for patches\n",
    "        self.positional_encoding = nn.Parameter(self.sinusoidal_positional_encoding_2d(image_size, image_size, embedding_dim)).to(device)\n",
    "        self.positional_encoding = self.positional_encoding.detach()\n",
    "        self.positional_encoding.requires_grad_(False)\n",
    "\n",
    "        # Feed Forward Layer\n",
    "        self.ffn1 = nn.Conv2d(embedding_dim, embedding_dim * 8, kernel_size=1)\n",
    "        self.ffn2 = nn.Conv2d(embedding_dim * 8, embedding_dim, kernel_size=1)\n",
    "\n",
    "    def sinusoidal_positional_encoding_2d(self, height: int, width: int, channel: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate a 2D sinusoidal positional encoding.\n",
    "\n",
    "        :param height: The height of the encoding.\n",
    "        :param width: The width of the encoding.\n",
    "        :param channel: The number of channels in the encoding.\n",
    "        :return: A tensor of shape (height, width, channel) containing the 2D positional encoding.\n",
    "        \"\"\"\n",
    "        if channel % 2 != 0:\n",
    "            raise ValueError(\"The 'channel' dimension must be an even number.\")\n",
    "\n",
    "        # First, build in (height, width, channel) format\n",
    "        pe = torch.zeros(height, width, channel)\n",
    "\n",
    "        half_ch = channel // 2\n",
    "\n",
    "        # Precompute the exponent for row and column\n",
    "        row_div_term = torch.exp(\n",
    "            -math.log(10000.0) * (torch.arange(0, half_ch, 2).float() / half_ch)\n",
    "        )\n",
    "        col_div_term = torch.exp(\n",
    "            -math.log(10000.0) * (torch.arange(0, half_ch, 2).float() / half_ch)\n",
    "        )\n",
    "\n",
    "        for h in range(height):\n",
    "            for w in range(width):\n",
    "                # Encode row index (h) into the first half of the channels\n",
    "                for i in range(0, half_ch, 2):\n",
    "                    pe[h, w, i]     = math.sin(h * row_div_term[i // 2])\n",
    "                    pe[h, w, i + 1] = math.cos(h * row_div_term[i // 2])\n",
    "\n",
    "                # Encode column index (w) into the second half of the channels\n",
    "                for j in range(0, half_ch, 2):\n",
    "                    pe[h, w, half_ch + j]     = math.sin(w * col_div_term[j // 2])\n",
    "                    pe[h, w, half_ch + j + 1] = math.cos(w * col_div_term[j // 2])\n",
    "\n",
    "        # Permute to get the shape (channel, width, height).\n",
    "        # Currently pe is (height, width, channel) = (H, W, C)\n",
    "        # We want (C, W, H), so we do permute(2, 1, 0).\n",
    "        pe = pe.permute(2, 1, 0)  # => (channel, width, height)\n",
    "\n",
    "        return pe\n",
    "\n",
    "    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        skip_tensor = tensor\n",
    "\n",
    "        tensor = self.gnorm1(tensor)\n",
    "\n",
    "        # Reshape for self attention\n",
    "        batch_size, channel, height, width = tensor.shape\n",
    "        tensor = tensor + self.positional_encoding\n",
    "        tensor = tensor.view(batch_size, channel, self.num_pixel)\n",
    "        tensor = tensor.permute(0, 2, 1)\n",
    "\n",
    "        tensor = self.qkv_proj(tensor)\n",
    "\n",
    "        query, key, value = torch.chunk(tensor, 3, dim=-1)\n",
    "        query = query.view(batch_size, self.num_pixel, self.num_head, self.head_dim)\n",
    "        key = key.view(batch_size, self.num_pixel, self.num_head, self.head_dim)\n",
    "        value = value.view(batch_size, self.num_pixel, self.num_head, self.head_dim)\n",
    "\n",
    "        query = query.transpose(1, 2)\n",
    "        key = key.transpose(1, 2)\n",
    "        value = value.transpose(1, 2)\n",
    "\n",
    "        # Self attention\n",
    "        attention_raw = torch.matmul(query, key.transpose(2, 3))\n",
    "        attention_scaled = attention_raw * self.scale\n",
    "        attention_score = torch.softmax(attention_scaled, dim=-1)\n",
    "        value = torch.matmul(attention_score, value)\n",
    "\n",
    "        # Reshape for self attention output\n",
    "        tensor = value.transpose(1, 2).contiguous()\n",
    "        tensor = tensor.view(batch_size, self.num_pixel, channel)\n",
    "        tensor = tensor.permute(0, 2, 1)\n",
    "        tensor = tensor.reshape(batch_size, channel, height, width)\n",
    "        tensor = self.output(tensor)\n",
    "\n",
    "        tensor = tensor + skip_tensor\n",
    "\n",
    "        # Feed Forward Layer\n",
    "        tensor = self.gnorm2(tensor)\n",
    "        tensor = self.ffn1(tensor)\n",
    "        tensor = F.relu(tensor)\n",
    "        tensor = self.ffn2(tensor)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "class SuperResolution(nn.Module):\n",
    "    def __init__(self, embedding_dim: list[int] = [3, 256, 256], input_image_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.module_list = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(embedding_dim) - 1):\n",
    "            self.module_list.append(ResBlock(in_channel=embedding_dim[i], out_channel=embedding_dim[i+1]))\n",
    "            self.module_list.append(SelfAttentionBlock(embedding_dim=embedding_dim[i+1], image_size=input_image_size * 2**i))\n",
    "            self.module_list.append(ResBlock(in_channel=embedding_dim[i+1], out_channel=embedding_dim[i+1], up=True))\n",
    "        self.module_list.append(ResBlock(in_channel=embedding_dim[-1], out_channel=3))\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        for module in self.module_list:\n",
    "            tensor = module(tensor)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4e3a4c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This model has 6912358 parameters.\n"
     ]
    }
   ],
   "source": [
    "super_resolution_net = SuperResolution().to(device)\n",
    "print(\"This model has\", sum(p.numel() for p in super_resolution_net.parameters()), \"parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96688b1",
   "metadata": {},
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba78659f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image transformations for preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "def valid_image_folder(path: str) -> bool:\n",
    "    # Check if file starts with '._' or ends with '.DS_Store'\n",
    "    filename = os.path.basename(path)\n",
    "    if filename.startswith(\"._\") or filename == \".DS_Store\": # Stupid MacOS\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Use ImageFolder to automatically label images based on folder names\n",
    "dataset = datasets.ImageFolder(root=dataset_filepath, is_valid_file=valid_image_folder, transform=transform)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Create DataLoaders for training and validation\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf1434",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7a53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 1\n",
    "lr = 3e-4\n",
    "weight_decay = 1e-3\n",
    "min_lr = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb4f5c7f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'amp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scaler \u001b[38;5;241m=\u001b[39m \u001b[43mamp\u001b[49m\u001b[38;5;241m.\u001b[39mGradScaler()\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39mAdamW(unet\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m lr, weight_decay \u001b[38;5;241m=\u001b[39m weight_decay)\n\u001b[1;32m      3\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'amp' is not defined"
     ]
    }
   ],
   "source": [
    "scaler = amp.GradScaler()\n",
    "optimizer = opt.AdamW(unet.parameters(), lr = lr, weight_decay = weight_decay)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs*len(train_dataloader), eta_min=min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0cd3b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "loss_train = []\n",
    "loss_valid = []\n",
    "for _ in epochs:\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    for images, _ in tqdm(train_loader):\n",
    "        low_resolution_image = F.interpolate(images, size=(64, 64), mode='bicubic', align_corners=False).to(device)\n",
    "        high_resolution_image = images.to(device)\n",
    "\n",
    "        # Typical pytorch training\n",
    "        optimizer.zero_grad()\n",
    "        with amp.autocast():\n",
    "            predicted_high_resolution_image = super_resolution_net(low_resolution_image)\n",
    "            loss = criterion(predicted_high_resolution_image, high_resolution_image)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        torch.nn.utils.clip_grad_norm_(super_resolution_net.parameters(), max_norm=1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Record loss\n",
    "        train_loss_list.append(loss.item())\n",
    "\n",
    "        # Step the learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, _ in tqdm(val_loader):\n",
    "            low_resolution_image = F.interpolate(images, size=(64, 64), mode='bicubic', align_corners=False).to(device)\n",
    "            high_resolution_image = images.to(device)\n",
    "        \n",
    "            with amp.autocast():\n",
    "                predicted_high_resolution_image = super_resolution_net(low_resolution_image)\n",
    "                loss = criterion(predicted_high_resolution_image, high_resolution_image)\n",
    "                \n",
    "            valid_loss_list.append(loss.item())\n",
    "            \n",
    "    print(f\"Epoch #{epoch}\")\n",
    "    print(f\"Current learning rate is {optimizer.param_groups[0]['lr']}\")\n",
    "    print(\"Train Loss is:\", sum(train_loss_list)/len(train_loss_list))\n",
    "    loss_train.append(sum(train_loss_list)/len(train_loss_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
